# spring-ai-search-engine

> An Enterprise-grade, Event-Driven AI Search Engine powered by Spring Boot 3.3+, Spring AI, and Kubernetes.

[![Java](https://img.shields.io/badge/Java-21-orange)](https://openjdk.org/projects/jdk/21/)
[![Spring Boot](https://img.shields.io/badge/Spring_Boot-3.3+-green)](https://spring.io/projects/spring-boot)
[![Spring AI](https://img.shields.io/badge/Spring_AI-1.0-blue)](https://spring.io/projects/spring-ai)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-Ready-326ce5)](https://kubernetes.io/)
[![Kafka](https://img.shields.io/badge/Apache_Kafka-Event--Driven-black)](https://kafka.apache.org/)

---

## ğŸ“– Overview

**Spring AI Search Engine** is a modular, microservices-based intelligent search platform demonstrating a production-grade **AI-Driven Search Pipeline** in Java.

The system evolves beyond simple RAG ("chat with documents") into a full **multi-stage search pipeline** inspired by real-world systems like LINE MAN Wongnai's search evolution â€” from keyword matching to semantic, intent-aware retrieval with LLM-powered reranking.

### Core Pipeline

```mermaid
flowchart TD
    A([User Query]) --> B[search-service]

    subgraph B[search-service]
        S1[1 Â· Query Expansion â€” LLM rewrite]
        S2[2 Â· Hybrid Retrieval â€” Vector + BM25 + RRF]
        S3[3 Â· LLM Reranker â€” LLM scoring]
        S4[4 Â· Answer Generation â€” RAG]
        S1 --> S2 --> S3 --> S4
    end

    VS[(Vector Store\nPGVector / Weaviate)] -->|top-K docs| S2
    IS[ingestion-service\nLoad â†’ Chunk â†’ Embed â†’ Kafka] -->|embeddings| VS

    S4 --> R([Search Response])
```

---

## ğŸ— Architecture

### Full System Architecture

```mermaid
flowchart TD
    U([User]) --> GW[API Gateway\nREST / Spring Cloud]

    GW --> IS[ingestion-service]
    GW --> SS[search-service]

    subgraph IS[ingestion-service]
        I1[1. Load Document]
        I2[2. Chunk]
        I3[3. Embed]
        I4[4. Publish to Kafka]
        I1 --> I2 --> I3 --> I4
    end

    subgraph SS[search-service]
        P1[1. Query Expand]
        P2[2. Hybrid Search]
        P3[3. LLM Rerank]
        P4[4. RAG Answer]
        P1 --> P2 --> P3 --> P4
    end

    I4 --> K[[Apache Kafka\ntopic: raw-docs]]
    K --> VS[(Vector Store\nPGVector / Weaviate)]
    VS --> P2
```

### Search Pipeline Detail

```mermaid
flowchart TD
    Q([Query Input]) --> E

    E["ğŸ§  QueryExpansionService\nLLM rewrites query into 2â€“3 semantic variants"]
    E -->|query variants| H

    subgraph H["ğŸ” HybridRetrievalService"]
        V[Dense Â· VectorStore.similaritySearch\ntop-20]
        B[Sparse Â· Elasticsearch BM25 / PG FTS\ntop-20]
        RRF[RRF Merge\nReciprocal Rank Fusion â†’ top-20]
        V --> RRF
        B --> RRF
    end

    H -->|top-20 candidates| R

    R["ğŸ† LLMReranker  âœ… LLM-only Â· kept\nLLM scores each doc vs query\nReturns top-5"]

    R -->|top-5 reranked docs| G

    G["âœï¸ AnswerGenerationService\nRAG Â· LLM generates grounded answer"]

    G --> Out([Search Response\nanswer + sources])
```

---

## ğŸš€ Key Features

- **âš¡ Event-Driven Ingestion** â€” uploads are async; heavy processing (PDF parsing, OCR, embedding) doesn't block the user API
- **ğŸ” Hybrid Search** â€” combines dense vector search with sparse keyword search (BM25), merged via Reciprocal Rank Fusion (RRF) for best-of-both coverage
- **ğŸ§  Query Expansion** â€” LLM rewrites ambiguous queries into multiple variants before retrieval, improving recall
- **ğŸ† LLM Reranking** â€” after retrieval, an LLM re-scores all candidates against the original query and returns only the most relevant results
- **âœï¸ RAG Answer Generation** â€” final answer is grounded in the top-N reranked documents
- **ğŸ³ Kubernetes Native** â€” Deployment, Service, and HPA manifests for production scaling
- **ğŸ“Š Observability** â€” token usage metrics (Micrometer) and distributed tracing (OpenTelemetry)

---

## ğŸ›  Tech Stack

| Layer | Technology |
|---|---|
| Language | Java 21 |
| Framework | Spring Boot 3.3+, Spring AI 1.0 |
| Messaging | Apache Kafka |
| Vector Store | PGVector (primary) / Weaviate (optional) |
| Keyword Search | Elasticsearch (BM25) / PG Full-Text Search |
| LLM Provider | OpenAI / Azure OpenAI / Ollama (local) |
| Orchestration | Kubernetes (K8s) |
| Observability | Micrometer + OpenTelemetry |

---

## ğŸ“¦ Module Structure

```
spring-ai-search-engine/
â”œâ”€â”€ ingestion-service/
â”‚   â”œâ”€â”€ controller/
â”‚   â”‚   â””â”€â”€ IngestionController.java        # POST /ingest
â”‚   â”œâ”€â”€ service/
â”‚   â”‚   â”œâ”€â”€ DocumentLoaderService.java      # PDF / HTML / text parsing
â”‚   â”‚   â”œâ”€â”€ ChunkingService.java            # TokenTextSplitter with overlap
â”‚   â”‚   â”œâ”€â”€ EmbeddingService.java           # Spring AI EmbeddingClient
â”‚   â”‚   â””â”€â”€ KafkaDocumentPublisher.java     # Publish to raw-docs topic
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ IngestionConfig.java
â”‚
â”œâ”€â”€ search-service/
â”‚   â”œâ”€â”€ controller/
â”‚   â”‚   â””â”€â”€ SearchController.java           # GET/POST /search
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ QueryExpansionService.java      # LLM query rewrite (NEW)
â”‚   â”‚   â”œâ”€â”€ HybridRetrievalService.java     # Vector + BM25 + RRF merge (NEW)
â”‚   â”‚   â”œâ”€â”€ LLMReranker.java                # LLM-only reranking (KEEP)
â”‚   â”‚   â””â”€â”€ AnswerGenerationService.java    # RAG generation (ENHANCED)
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ SearchPipelineConfig.java
â”‚
â”œâ”€â”€ shared/                                 # NEW: shared library
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ SearchRequest.java
â”‚   â”‚   â”œâ”€â”€ SearchResult.java
â”‚   â”‚   â””â”€â”€ RankedDocument.java
â”‚   â””â”€â”€ util/
â”‚       â””â”€â”€ RRFMerger.java                  # Reciprocal Rank Fusion
â”‚
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”œâ”€â”€ infra/                              # Kafka, PGVector, Elasticsearch
â”‚   â””â”€â”€ apps/                              # ingestion-service, search-service
â”‚
â””â”€â”€ docker-compose.yml
```

---

## ğŸƒ Getting Started

### Prerequisites

- Java 21+
- Docker & Docker Compose
- Maven 3.9+

### Environment Variables

```bash
# LLM Provider (choose one)
OPENAI_API_KEY=sk-...
# or for local
OLLAMA_BASE_URL=http://localhost:11434

# Vector Store
SPRING_AI_VECTORSTORE_PGVECTOR_URL=jdbc:postgresql://localhost:5432/vectordb
SPRING_AI_VECTORSTORE_PGVECTOR_USERNAME=postgres
SPRING_AI_VECTORSTORE_PGVECTOR_PASSWORD=secret

# Kafka
SPRING_KAFKA_BOOTSTRAP_SERVERS=localhost:9092

# Elasticsearch (for BM25 hybrid search)
ELASTICSEARCH_URL=http://localhost:9200
```

### Local Development

```bash
# 1. Clone
git clone https://github.com/Peqchji/spring-ai-search-engine.git
cd spring-ai-search-engine
git checkout develop

# 2. Start infrastructure
docker-compose up -d
# Starts: Kafka, PostgreSQL+pgvector, Elasticsearch, Ollama (optional)

# 3. Build all modules
mvn clean install

# 4. Run services
# Terminal A â€” Ingestion
java -jar ingestion-service/target/ingestion-service.jar

# Terminal B â€” Search
java -jar search-service/target/search-service.jar
```

### Quick Test

```bash
# Ingest a document
curl -X POST http://localhost:8080/ingest \
  -F "file=@/path/to/document.pdf"

# Search
curl -X POST http://localhost:8081/search \
  -H "Content-Type: application/json" \
  -d '{"query": "What is the refund policy?"}'
```

---

## â˜¸ï¸ Kubernetes Deployment

```bash
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/infra/      # Kafka, PGVector, Elasticsearch
kubectl apply -f k8s/apps/       # ingestion-service, search-service
```

---

## ğŸ—º Development Roadmap

| Phase | Goal | Status |
|---|---|---|
| 1 | Stabilize ingestion + basic vector search end-to-end | âœ… In Progress |
| 2 | Add `QueryExpansionService` (LLM query rewrite) | ğŸ”² Planned |
| 3 | Add hybrid retrieval + RRF merge | ğŸ”² Planned |
| 4 | Harden `LLMReranker` (prompt tuning, batching) | ğŸ”² Planned |
| 5 | Observability: token cost tracking, pipeline tracing | ğŸ”² Planned |

---

## ğŸ“„ Documentation

- [`AGENTIC.md`](./AGENTIC.md) â€” Detailed breakdown of agentic pipeline stages, LLM reranker design, and prompt templates

---

## ğŸ“œ License

MIT License. See [LICENSE](./LICENSE) for details.
